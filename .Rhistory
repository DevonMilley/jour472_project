# make a zip5 column and redo the join
maryland_ppp_with_naics <- maryland_ppp_with_naics %>% mutate(zip5 = str_sub(zip, 1, 5))
maryland_ppp_with_naics_and_demographics <- maryland_ppp_with_naics %>% left_join(maryland_zcta, by=c("zip5"="ZCTA5N"))
# zcta with > 50% non-Hispanic Black population
maryland_ppp_with_naics_and_demographics %>%
filter(PNHB > 50) %>%
summarize(
count = n(),
avgamount = mean(amount),
medamount = median(amount))
# zcta with > 50% non-Hispanic white population
maryland_ppp_with_naics_and_demographics %>%
filter(PNHW > 50) %>%
summarize(
count = n(),
avgamount = mean(amount),
medamount = median(amount))
# zcta with > 50% non-Hispanic Black population grouped by rural/urban
maryland_ppp_with_naics_and_demographics %>%
filter(PNHB > 50) %>%
group_by(rural_urban_indicator) %>%
summarize(
count = n(),
avgamount = mean(amount),
medamount = median(amount))
# zcta with > 50% non-Hispanic white population grouped by rural/urban
maryland_ppp_with_naics_and_demographics %>%
filter(PNHW > 50) %>%
group_by(rural_urban_indicator) %>%
summarize(
count = n(),
avgamount = mean(amount),
medamount = median(amount))
knitr::opts_chunk$set(echo = TRUE)
install.packages('refinr')
install.packages('refinr')
# turn off sci notation
options(scipen=999)
library(tidyverse)
library(lubridate)
library(refinr)
# turn off sci notation
options(scipen=999)
library(tidyverse)
library(lubridate)
library(refinr)
# Load data
md_loans <- read_csv("data/ppp_loans_md.csv.zip")
# Load data
md_loans <- read_csv("data/ppp_loans_md.csv.zip")
# Display it
md_loans
# Now let's try and group and count the number of loans by city. To make it a bit more managable, let's use another string function from `stringr` and filter for cities that start with the uppercase "A" or lowercase "a" using the function `str_detect()` with a regular expression.
# The filter function in the codeblock below says: look in the city column, and pluck out any value that starts with (the "^" symbol means "starts with") a lowercase "a" OR (the vertical "|", called a pipe, means OR) an uppercase "A".
md_loans %>%
group_by(city) %>%
summarise(
count=n()
) %>%
filter(str_detect(city, "^a|^A")) %>%
arrange(city)
cleaned_md_loans <- md_loans %>%
mutate(city_clean=key_collision_merge(city)) %>%
select(id:city, city_clean, everything())
cleaned_md_loans
cleaned_md_loans %>%
group_by(city_clean, city) %>%
summarise(
count=n()
) %>%
filter(str_detect(city, "^a|^A")) %>%
arrange(city)
cleaned_md_loans <- md_loans %>%
mutate(city_clean=n_gram_merge(city)) %>%
select(id:city, city_clean, everything())
cleaned_md_loans %>%
group_by(city_clean, city) %>%
summarise(
count=n()
) %>%
filter(str_detect(city, "^a|^A")) %>%
arrange(city)
knitr::opts_chunk$set(echo = TRUE)
install.packages("tidyverse")
options(scipen=999)
install.packages("janitor")
library(tidyverse)
install.packages("tidyverse")
options(scipen=999)
install.packages("janitor")
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
options(scipen=999)
install.packages("janitor")
knitr::opts_chunk$set(echo = TRUE)
install.packages("tidyverse")
options(scipen=999)
install.packages("janitor")
library(tidyverse)
install.packages("tidyverse")
options(scipen=999)
install.packages("janitor")
library(tidyverse)
# Load required data
la_payouts <- read.csv("data/la_city_legal_payouts.csv")
View(la_payouts)
# Clean required data and prepare for analysis if needed.
la_payouts <- la_payouts %>% mutate(AMOUNT=as.numeric(parse_number(AMOUNT)))
#sum payouts for 2018
la_payouts%>%
group_by(Year)%>%
summarize(
count = n(),
total = sum(AMOUNT))
# Load required data
la_payouts <- read.csv("data/la_city_legal_payouts.csv")
View(la_payouts)
# Clean required data and prepare for analysis if needed.
la_payouts <- la_payouts %>% mutate(AMOUNT=as.numeric(parse_number(AMOUNT)))
# Put code to reverse engineer sentence here
#sum payouts for 2018
la_payouts%>%
group_by(Year)%>%
summarize(
count = n(),
total = sum(AMOUNT))
# Display results of code below this codeblock
# Put code to reverse engineer sentence here
#sum payouts for 2018
la_payouts%>%
group_by(Year)%>%
summarize(
count = n(),
total = sum(AMOUNT)) %>%
arrange(desc(total))
# Display results of code below this codeblock
knitr::opts_chunk$set(echo = TRUE)
# Create an empty dataframe to hold results
employment_by_sector_all <- tibble()
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(tidyverse)
library(janitor)
knitr::include_graphics(rep("images/rvest1.png"))
naics_url <- "https://www.bls.gov/ces/naics/"
knitr::include_graphics(rep("images/rvest2.png"))
# read in the html
naics_industry <- naics_url %>%
read_html()
# display the html below
naics_industry
knitr::include_graphics(rep("images/rvest3.png"))
knitr::include_graphics(rep("images/rvest4.png"))
# read in the html and extract all the tables
naics_industry <- naics_url %>%
read_html() %>%
html_table()
# display the tables below
naics_industry
knitr::include_graphics(rep("images/rvest5.png"))
knitr::include_graphics(rep("images/rvest6.png"))
knitr::include_graphics(rep("images/rvest7.png"))
# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
read_html() %>%
html_table()
# Just keep the second dataframe in our list
naics_industry <- naics_industry[[2]]
# show the dataframe
naics_industry
# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry <- naics_url %>%
read_html() %>%
html_table()
# Just keep the second dataframe in our list, standardize column headers, remove last row
naics_industry <- naics_industry[[2]] %>%
clean_names() %>%
slice(-21)
# show the dataframe
naics_industry
library(rvest)
library(tidyverse)
library(janitor)
# Define url of page we want to scrape
naics_url <- "https://www.bls.gov/ces/naics/"
# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
read_html() %>%
html_table()
# Just keep the second dataframe in our list, standardize column headers, remove last row
naics_industry <- naics_industry[[2]] %>%
clean_names() %>%
slice(-21)
# show the dataframe
naics_industry
knitr::include_graphics(rep("images/advrvest1a.png"))
knitr::include_graphics(rep("images/advrvest1.png"))
# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"
# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"
# Get employment html
employment_info <- url %>%
read_html()
# Display it so we can see what it looks like
employment_info
knitr::include_graphics(rep("images/advrvest2.png"))
# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"
# Get employment html page and select only the table with employment information
employment_info <- url %>%
read_html() %>%
html_element(xpath = '//*[@id="iag22emp1"]')
# Display it so we can see what it looks like
employment_info
# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"
# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
read_html() %>%
html_element(xpath = '//*[@id="iag22emp1"]') %>%
html_table()
# Display it so we can see what it looks like
employment_info
# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"
# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
read_html() %>%
html_element(xpath = '//*[@id="iag21emp1"]') %>%
html_table()
# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info %>%
clean_names() %>%
slice(2) %>%
select(data_series, nov_2021)
# Display it so we can see what it looks like
employment_info
print("Agriculture, Forestry, Fishing and Hunting")
print("Mining, Quarrying, and Oil and Gas Extraction")
print("Utilities")
print("Construction")
print("Manufacturing")
print("Wholesale Trade")
print("Retail Trade")
print("Transportation and Warehousing")
print("Information")
print("Finance and Insurance")
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")
# Make a for loop and run it
for (sector in list_of_sectors) {
print(sector)
}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")
# For loop that refers to a list that doesn't exist!
for (sector in sector_list) {
print(sector)
}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")
# For loop with x that stands in for each element in our list, instead of sector
for (x in list_of_sectors) {
print(x)
}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")
# For loop that includes instructions that refer to a variable that doesn't exist.
for (x in list_of_sectors) {
print(sector_name)
}
# For loop that includes instructions that refer to a variable that doesn't exist.
for (number in 1:10) {
print(number)
}
naics_industry
nrow(naics_industry)
# For loop that includes instructions that refer to a variable that doesn't exist.
for (row_number in 1:nrow(naics_industry)) {
print(row_number)
}
# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"
# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
read_html() %>%
html_element(xpath = '//*[@id="iag21emp1"]') %>%
html_table()
# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info %>%
clean_names() %>%
slice(2) %>%
select(data_series, nov_2021)
# Display it so we can see what it looks like
employment_info
# Define url of page we want to scrape
naics_url <- "https://www.bls.gov/ces/naics/"
# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
read_html() %>%
html_table()
# Just keep the second dataframe in our list, standardize column headers, remove last row
naics_industry <- naics_industry[[2]] %>%
clean_names() %>%
slice(-21)
# show the dataframe
naics_industry
# Make a column with URL for each sector.
naics_industry <- naics_industry %>%
mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm"))
# Display it
naics_industry
# Make a column with URL and xpath ID for each sector
naics_industry <- naics_industry %>%
mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) %>%
mutate(sector_xpath_id =paste0("iag",sector,"emp1"))
# Display it
naics_industry
# Make a column with URL and xpath ID for each sector, remove the Public Administration sector
naics_industry <- naics_industry %>%
mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) %>%
mutate(sector_xpath_id =paste0("iag",sector,"emp1")) %>%
filter(description != "Public Administration")
# Display it
naics_industry
# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- naics_industry %>%
slice(row_number)
# To help us see what's happening as we build this, we're going to print the thing we're creating.
print(each_row_df)
}
# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- naics_industry %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$sector_url
# Define id of table to ingest
xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
# To help us see what's happening as we build this, we're going to print the thing we're creating.
print(paste0("ROW NUMBER:", row_number," URL: ",url," XPATH:",xpath_employment_table))
}
# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- naics_industry %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$sector_url
# Define id of table to ingest
xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
# Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table(). The dataframe is in a nested list, which we'll have to extract in the next step.
employment_info <- url %>%
read_html() %>%
html_elements(xpath = xpath_employment_table) %>%
html_table()
# To help us see what's happening as we build this, we're going to print the thing we're creating.
print(employment_info)
}
# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- naics_industry %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$sector_url
# Define id of table to ingest
xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
# Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
employment_info <- url %>%
read_html() %>%
html_elements(xpath = xpath_employment_table) %>%
html_table()
# Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row;
employment_info <- employment_info[[1]] %>%
clean_names() %>%
slice(2)
# To help us see what's happening as we build this, we're going to print the thing we're creating.
print(employment_info)
}
# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- naics_industry %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$sector_url
# Define id of table to ingest
xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
# Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
employment_info <- url %>%
read_html() %>%
html_elements(xpath = xpath_employment_table) %>%
html_table()
# Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table.
employment_info <- employment_info[[1]] %>%
clean_names() %>%
slice(2) %>%
bind_cols(each_row_df)
# To help us see what's happening as we build this, we're going to print the thing we're creating.
print(employment_info)
}
# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- naics_industry %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$sector_url
# Define id of table to ingest
xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
# Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
employment_info <- url %>%
read_html() %>%
html_elements(xpath = xpath_employment_table) %>%
html_table()
# Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jun_2021 column into a proper number, and rename it.  Then select only three columns we need.
employment_info <- employment_info[[1]] %>%
clean_names() %>%
slice(2) %>%
bind_cols(each_row_df) %>%
mutate(nov_2021 = parse_number(nov_2021)) %>%
rename(nov_2021_employees = nov_2021) %>%
select(sector,description,nov_2021_employees)
# To help us see what's happening as we build this, we're going to print the thing we're creating.
print(employment_info)
}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
library(tidyverse)
library(janitor)
# Load required data
la_payouts <- read.csv("data/la_city_legal_payouts.csv")
View(la_payouts)
# Clean required data and prepare for analysis if needed.
la_payouts <- la_payouts %>% mutate(AMOUNT=as.numeric(parse_number(AMOUNT)))
# Put code to reverse engineer sentence here
#sum payouts for 2018
la_payouts%>%
group_by(Year)%>%
summarize(
count = n(),
total = sum(AMOUNT)) %>%
arrange(desc(total))
# Display results of code below this codeblock
# Put code to reverse engineer sentence here
la_payouts_departments<-la_payouts%>%
group_by(DEPARTMENT)%>%
summarize(
case_count = n(),
total_amount = sum(AMOUNT)) %>%
arrange(desc(total_amount))
la_payouts%>%
filter(AMOUNT > 0) %>%
summarize(
total_amount_of_all_cases = sum(AMOUNT))
la_payouts_pct_share <- la_payouts_departments%>%
mutate(pct_share =(total_amount/886979072)*100)
la_payouts_wrong_imprisonment <- la_payouts %>%
filter(DEPARTMENT=="Police Department" & CASE.TYPE == "Police - False Arrest/Detention/Imprisonment") %>%
summarise(
AMOUNT
) %>%
arrange(desc(AMOUNT))
first_two_cases_wrong_imprisonment <- la_payouts_wrong_imprisonment %>%
summarise(
AMOUNT[1] + AMOUNT[2]
)
# Display results of code below this codeblock
first_two_cases_wrong_imprisonment
la_payouts_pct_share
# Put code to reverse engineer sentence here
la_payouts%>%
filter(Year == 2018, (str_detect(CASE.TYPE, 'Dangerous Condition')))%>%
summarize(
count = n(),
total = sum(AMOUNT)) %>%
arrange(desc(total))
la_payouts%>%
filter(Year == 2018 , DEPARTMENT == 'Police Department')%>%
summarize(
count = n(),
total = sum(AMOUNT))%>%
arrange(desc(total))
# Display results of code below this codeblock
# Put code to reverse engineer sentence here
la_payouts%>%
filter(Year == 2018, (str_detect(CASE.TYPE, 'Dangerous Condition')))%>%
summarize(
count = n(),
total = sum(AMOUNT)) %>%
arrange(desc(total))
# Display results of code below this codeblock
# Put code to reverse engineer sentence here
la_payouts%>%
filter(Year == 2018 , DEPARTMENT == 'Police Department')%>%
summarize(
count = n(),
total = sum(AMOUNT))%>%
arrange(desc(total))
# Display results of code below this codeblock
